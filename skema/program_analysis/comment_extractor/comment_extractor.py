import argparse
from typing import List, Dict, Tuple, Union, Optional
from pathlib import Path

import yaml
from tree_sitter import Language, Parser, Node
from pydantic import BaseModel, Field

from skema.program_analysis.comment_extractor.model import (
    SingleLineComment,
    MultiLineComment,
    Docstring,
    SingleFileCommentRequest,
    SingleFileCommentResponse,
    MultiFileCommentRequest,
    MultiFileCommentResponse,
    SupportedLanguage,
    SupportedLanguageResponse,
)
from skema.program_analysis.tree_sitter_parsers.build_parsers import (
    INSTALLED_LANGUAGES_FILEPATH,
)


def get_identifier(node: Node, source: str):
    """Given a tree-sitter node object, return the string representing the source between node.start_point and node.end_point"""
    line_num = 0
    column_num = 0
    in_identifier = False
    identifier = ""
    for i, char in enumerate(source):
        if line_num == node.start_point[0] and column_num == node.start_point[1]:
            in_identifier = True
        elif line_num == node.end_point[0] and column_num == node.end_point[1]:
            break

        if char == "\n":
            line_num += 1
            column_num = 0
        else:
            column_num += 1

        if in_identifier:
            identifier += char

    return identifier


def node_to_single_line_comment(node: Node, source: str) -> SingleLineComment:
    """Converts a tree-sitter node to a SingleLineComment object"""
    content = get_identifier(node, source)
    line_number = node.start_point[0]

    return SingleLineComment(content=content, line_number=line_number)


def node_to_multi_line_comment_partial(nodes: List[Node], source: str):
    """Converts a list of tree-sitter nodes to a single MultiLineCodeComment object"""
    content = [get_identifier(node, source) for node in nodes]
    start_line_number = nodes[0].start_point[0]
    end_line_number = nodes[-1].end_point[0]

    return MultiLineComment(
        content=content,
        start_line_number=start_line_number,
        end_line_number=end_line_number,
    )


def node_to_multi_line_comment(node: Node, source: str) -> Dict:
    """Converts a tree-sitter node to a MultiLineComment object"""
    content = get_identifier(node, source).split("\n")
    start_line_number = node.start_point[0]
    end_line_number = node.end_point[0]

    return MultiLineComment(
        content=content,
        start_line_number=start_line_number,
        end_line_number=end_line_number,
    )


def nodes_to_docstring_partial(name_node: Node, content_nodes: List[Node], source: str):
    """Converts a list of tree-sitter nodes to a single Docstring object"""
    content = [get_identifier(node, source) for node in content_nodes]
    function_name = get_identifier(name_node, source)
    start_line_number = content_nodes[0].start_point[0]
    end_line_number = content_nodes[-1].end_point[0]

    return Docstring(
        content=content,
        function_name=function_name,
        start_line_number=start_line_number,
        end_line_number=end_line_number,
    )


def nodes_to_docstring(name_node: Node, content_node: Node, source: str) -> Dict:
    """Converts a tree-sitter node to a Docstring object"""
    content = get_identifier(content_node, source).split("\n")
    function_name = get_identifier(name_node, source)
    start_line_number = content_node.start_point[0]
    end_line_number = content_node.end_point[0]

    return Docstring(
        content=content,
        function_name=function_name,
        start_line_number=start_line_number,
        end_line_number=end_line_number,
    )


def preprocess_captures(captures: List[Tuple[Node, str]]) -> List[Tuple[Node, str]]:
    """Preprocess list of captures generated by tree-sitter. This preprocessing includes:
    1. Reording captures to go docstring -> multi -> single
    2. Moving docstring name node after docstring body node in languages with internal function docstrings
    3. Removing duplicate nodes captured my multiple capture groups
    """

    def hash_node(node: Node):
        """Create a hashable tuple of tree-sitter node."""
        return (node.type, node.start_point, node.end_point)

    # Rearrange the captures to go Docstring -> Multi -> Single
    order = {
        "docstring_body": 1,
        "docstring_name": 1,
        "docstring_body_partial": 1,
        "multi": 2,
        "multi_partial": 2,
        "single": 3,
    }
    ordered = sorted(captures, key=lambda capture: (order[capture[1]]))

    # The order of docstring_body and docstring_name can differ between languages
    # To standardize this, we will the docstring_name node to the back.
    # We do this by additionally sorting on the line number.
    if len(ordered) > 0 and ordered[0][1] == "docstring_name":
        ordered = ordered = sorted(
            captures, key=lambda capture: (order[capture[1]], -capture[0].start_byte)
        )

    output = []
    duplicates = set()
    for comment in ordered:
        hash = hash_node(comment[0])
        if hash in duplicates:
            continue
        output.append(comment)
        duplicates.add(hash)

    return output


def extract_comments_single(
    request: SingleFileCommentRequest,
) -> SingleFileCommentResponse:
    # Get tree-sitter queries for given language
    queries_filepath = Path(__file__).parent / "queries.yaml"
    queries_obj = yaml.safe_load(open(queries_filepath))
    if request.language not in queries_obj:
        return None
    queries = queries_obj[request.language]

    language_obj = Language(INSTALLED_LANGUAGES_FILEPATH, request.language)

    # Parse source and run query with tree-sitter
    parser = Parser()
    parser.set_language(language_obj)
    tree = parser.parse(bytes(request.source, encoding="UTF-8"))
    captures = language_obj.query(queries_obj[request.language]).captures(
        tree.root_node
    )
    captures = preprocess_captures(captures)

    # Loop over captures, converting them to CodeComment objects.
    # There are currently 5 types of capture groups supported by the comment extarctor
    # 1. single - Single line comment (i.e. Fortran '!')
    # 2. multi - Multi line comment (i.e. C '/* */')
    # 3. multi_partial - Adjacent single line comments in languages that don't have a multi line comment token
    # 4. docstring_body - Docstring comment (def foo():\n""" """)
    # 5. docstring_body_partial - Adjacent single line docstring comments in languages that don't have a multi line comment token
    single, multi, docstring = ([], [], [])
    index = 0
    while index < len(captures):
        node, type = captures[index]
        if type == "single":
            single.append(node_to_single_line_comment(node, request.source))
        elif type == "multi":
            multi.append(node_to_multi_line_comment(node, request.source))
        elif type == "multi_partial":
            # For partial multi line comments, we have to determine the stopping point.
            # We do this by checking the the line number of each capture. If its >1 line away from the previous capture, we have hit the stopping point.
            multi_start_index = index
            multi_end_index = None
            for i in range(index, len(captures) - 1):
                current_line_number = captures[i][0].end_point[0]
                next_line_number = captures[i + 1][0].end_point[0]
                if next_line_number - current_line_number != 1:
                    multi_end_index = i + 1
                    break

            multi_body_nodes = [
                node for node, _ in captures[multi_start_index:multi_end_index]
            ]
            multi.append(
                node_to_multi_line_comment_partial(multi_body_nodes, request.source)
            )
            index = multi_end_index
            continue
        elif type == "docstring_body":
            docstring_name_node = captures[index + 1][0]
            docstring.append(
                nodes_to_docstring(docstring_name_node, node, request.source)
            )
            index += 2
            continue
        elif type == "docstring_body_partial":
            # Similar to partial multi line comments, partial docstrings also need a stopping index.
            # Due to the  preprocessing earlier, the stop index will always be the index of the next docstring_name node.
            docstring_name_index = -1
            for i in range(index, len(captures)):
                if captures[i][1] == "docstring_name":
                    docstring_name_index = i
                    break
            docstring_name_node = captures[docstring_name_index][0]
            docstring_body_nodes = [
                node for node, _ in captures[index:docstring_name_index]
            ]
            docstring.append(
                nodes_to_docstring_partial(
                    docstring_name_node, docstring_body_nodes, request.source
                )
            )
            index = docstring_name_index + 1
            continue
        index += 1

    return SingleFileCommentResponse(single=single, multi=multi, docstring=docstring)


def extract_comments_multi(
    request: MultiFileCommentRequest,
) -> MultiFileCommentResponse:
    """Wrapper for processing multiple source files at a time."""
    return MultiFileCommentResponse.parse_obj(
        {
            "files": {
                file_name: extract_comments_single(file_request)
                for file_name, file_request in request.files.items()
            }
        }
    )


def get_supported_languages() -> SupportedLanguageResponse:
    """Return the languages supported for comment extraction"""
    queries_filepath = Path(__file__).parent / "queries.yaml"
    queries_obj = yaml.safe_load(open(queries_filepath))

    languages = []
    for language, query in queries_obj.items():
        single = True if "@single" in query else False
        multi = True if "@multi" in query or "@multi_partial" in query else False
        docstring = (
            True
            if "@docstring_body" in query or "docstring_body_partial" in query
            else False
        )
        languages.append(
            SupportedLanguage(
                name=language, single=single, multi=multi, docstring=docstring
            )
        )

    return SupportedLanguageResponse(languages=languages)
