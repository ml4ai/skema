<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>skema.img2mml.translate API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>skema.img2mml.translate</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># -*- coding: utf-8 -*-

import random
import numpy as np
import torch
from torchvision import transforms
from PIL import Image
from skema.img2mml.models.encoders.cnn_encoder import CNN_Encoder
from skema.img2mml.models.encoders.xfmer_encoder import Transformer_Encoder
from skema.img2mml.models.decoders.xfmer_decoder import Transformer_Decoder
import io
from typing import List, Union
import logging
import re
from skema.img2mml.models.image2mml_xfmer import Image2MathML_Xfmer
from xml.etree import ElementTree as ET

# Set logging level to INFO
logging.basicConfig(level=logging.INFO)


def remove_eqn_number(image: Image.Image, threshold: float = 0.1) -&gt; Image.Image:
    &#34;&#34;&#34;
    Remove equation number from an image of an equation.

    Args:
        image (Image.Image): The input image.
        threshold (float, optional): The threshold to determine the size of the equation number.
            A smaller threshold will consider larger areas as equation numbers.
            Defaults to 0.1.

    Returns:
        Image.Image: The modified image with the equation number removed.
    &#34;&#34;&#34;
    image_arr = np.asarray(image, dtype=np.uint8)

    # Invert the image by subtracting it from the maximum pixel value
    inverted = np.max(image_arr) - image_arr

    # Get the width and height of the image
    height, width = inverted.shape[:2]

    # Start scanning from the right side
    column_sum = np.sum(inverted, axis=0)
    rightmost_column = width - 1
    leftmost_column = rightmost_column
    while leftmost_column &gt;= 0:
        if column_sum[leftmost_column] != 0:
            if rightmost_column - leftmost_column &gt; threshold * width:
                image_arr = image_arr[:, 0:leftmost_column]
                return Image.fromarray(image_arr)

            leftmost_column -= 1
            rightmost_column = leftmost_column
        else:
            leftmost_column -= 1

    return Image.fromarray(image_arr)


def calculate_scale_factor(
    image: Image.Image, target_width: int, target_height: int
) -&gt; float:
    &#34;&#34;&#34;
    Calculate the scale factor to normalize the input image to the target width and height while preserving the
    original aspect ratio. If the original aspect ratio is larger than the target aspect ratio, the scale factor
    will be calculated based on width. Otherwise, it will be calculated based on height.

    Args:
        image (PIL.Image.Image): The input image to be normalized.
        target_width (int): The target width for normalization.
        target_height (int): The target height for normalization.

    Returns:
        float: The scale factor to normalize the image.
    &#34;&#34;&#34;
    original_width, original_height = image.size
    original_aspect_ratio = original_width / original_height
    target_aspect_ratio = target_width / target_height

    if original_aspect_ratio &gt; target_aspect_ratio:
        # Calculate scale factor based on width
        scale_factor = target_width / original_width
    else:
        # Calculate scale factor based on height
        scale_factor = target_height / original_height

    return scale_factor


def preprocess_img(image: Image.Image, config: dict) -&gt; Image.Image:
    &#34;&#34;&#34;preprocessing image - cropping, resizing, and padding&#34;&#34;&#34;
    # remove equation number if having
    image = remove_eqn_number(image)

    # converting to np array
    image_arr = np.asarray(image, dtype=np.uint8)
    # find where the data lies
    indices = np.where(image_arr != 255)
    # get the boundaries
    x_min = np.min(indices[1])
    x_max = np.max(indices[1])
    y_min = np.min(indices[0])
    y_max = np.max(indices[0])

    # cropping tha image
    image = image.crop((x_min, y_min, x_max, y_max))

    # calculate the target width and height
    target_width = config[&#34;preprocessed_image_width&#34;] - 2 * config[&#34;padding&#34;]
    target_height = config[&#34;preprocessed_image_height&#34;] - 2 * config[&#34;padding&#34;]
    # calculate the scale factor
    resize_factor = calculate_scale_factor(image, target_width, target_height)

    # resizing the image
    image = image.resize(
        (
            int(image.size[0] * resize_factor),
            int(image.size[1] * resize_factor),
        ),
        Image.LANCZOS,
    )

    # padding
    pad = config[&#34;padding&#34;]
    width = config[&#34;preprocessed_image_width&#34;]
    height = config[&#34;preprocessed_image_height&#34;]
    new_image = Image.new(&#34;RGB&#34;, (width, height), (255, 255, 255))
    new_image.paste(image, (pad, pad))

    return new_image


def convert_to_torch_tensor(image: bytes, config: dict) -&gt; torch.Tensor:
    &#34;&#34;&#34;Convert image to torch tensor.&#34;&#34;&#34;
    image = Image.open(io.BytesIO(image)).convert(&#34;L&#34;)
    image = preprocess_img(image, config)

    # convert to tensor
    image = transforms.ToTensor()(image)

    return image


def set_random_seed(seed: int) -&gt; None:
    &#34;&#34;&#34;Set up seed.&#34;&#34;&#34;

    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)


def define_model(
    config: dict, vocab: List[str], device: torch.device, model_type=&#34;xfmer&#34;
) -&gt; Image2MathML_Xfmer:
    &#34;&#34;&#34;
    Defining the model
    initializing encoder, decoder, and model
    &#34;&#34;&#34;

    print(&#34;Defining model...&#34;)

    model_type = config[&#34;model_type&#34;]
    input_channels = config[&#34;input_channels&#34;]
    output_dim = len(vocab)
    emb_dim = config[&#34;embedding_dim&#34;]
    dec_hid_dim = config[&#34;decoder_hid_dim&#34;]
    dropout = config[&#34;dropout&#34;]
    max_len = config[&#34;max_len&#34;]

    print(f&#34;building {model_type} model...&#34;)

    dim_feedfwd = config[&#34;dim_feedforward_for_xfmer&#34;]
    n_heads = config[&#34;n_xfmer_heads&#34;]
    n_xfmer_encoder_layers = config[&#34;n_xfmer_encoder_layers&#34;]
    n_xfmer_decoder_layers = config[&#34;n_xfmer_decoder_layers&#34;]
    len_dim = 2500

    enc = {
        &#34;CNN&#34;: CNN_Encoder(input_channels, dec_hid_dim, dropout, device),
        &#34;XFMER&#34;: Transformer_Encoder(
            emb_dim,
            dec_hid_dim,
            n_heads,
            dropout,
            device,
            max_len,
            n_xfmer_encoder_layers,
            dim_feedfwd,
            len_dim,
        ),
    }
    dec = Transformer_Decoder(
        emb_dim,
        n_heads,
        dec_hid_dim,
        output_dim,
        dropout,
        max_len,
        n_xfmer_decoder_layers,
        dim_feedfwd,
        device,
    )
    model = Image2MathML_Xfmer(enc, dec, vocab, device)

    return model


def process_mtext(xml_string: str) -&gt; str:
    &#34;&#34;&#34;
    Process the input MathML string by merging consecutive &lt;mi&gt; elements and replacing specific &lt;mrow&gt; structures.

    Args:
        xml_string (str): The input MathML string.

    Returns:
        str: The modified MathML string.
    &#34;&#34;&#34;
    root = ET.fromstring(xml_string)

    def merge_mi_elements(elements):
        &#34;&#34;&#34;
        Merge consecutive &lt;mi&gt; elements into an &lt;mtext&gt; element.

        Args:
            elements (list): List of consecutive &lt;mi&gt; elements.

        Returns:
            Element: The merged &lt;mtext&gt; element.
        &#34;&#34;&#34;
        merged_text = &#34;&#34;.join([elem.text for elem in elements])
        mtext = ET.Element(&#34;mtext&#34;)
        mtext.text = merged_text
        return mtext

    # Replace specific &lt;mrow&gt; structures with &lt;mtext&gt;
    for mrow in root.findall(&#34;.//mrow&#34;):
        mi_count = sum(1 for child in mrow if child.tag == &#34;mi&#34;)
        non_mi_count = sum(1 for child in mrow if child.tag != &#34;mi&#34;)

        if mi_count &gt;= 3 and non_mi_count == 0:
            mi_elements = [child for child in mrow if child.tag == &#34;mi&#34;]
            mtext = merge_mi_elements(mi_elements)
            mrow.clear()
            mrow.tag = &#34;to_be_removed&#34;
            mrow.append(mtext)

    mi_count = 0
    consecutive_mi_elements = []
    new_children = []

    # Merge consecutive &lt;mi&gt; elements
    for child in root:
        if child.tag == &#34;mi&#34;:
            mi_count += 1
            consecutive_mi_elements.append(child)
        else:
            if mi_count &gt;= 5:
                mtext = merge_mi_elements(consecutive_mi_elements)
                new_children.append(mtext)
            else:
                new_children.extend(consecutive_mi_elements)
            mi_count = 0
            consecutive_mi_elements = []
            new_children.append(child)

    if mi_count &gt;= 5:
        mtext = merge_mi_elements(consecutive_mi_elements)
        new_children.append(mtext)
    else:
        new_children.extend(consecutive_mi_elements)

    root.clear()
    root.extend(new_children)

    modified_xml_string = ET.tostring(root, encoding=&#34;unicode&#34;)
    modified_xml_string = modified_xml_string.replace(&#34;&lt;to_be_removed&gt;&#34;, &#34;&#34;)
    modified_xml_string = modified_xml_string.replace(&#34;&lt;/to_be_removed&gt;&#34;, &#34;&#34;)

    return modified_xml_string


def add_semicolon_to_unicode(string: str) -&gt; str:
    &#34;&#34;&#34;
    Checks if the string contains Unicode starting with &#39;&amp;#x&#39; and adds a semicolon &#39;;&#39; after each occurrence if missing.

    Args:
        string (str): The input string to check.

    Returns:
        str: The modified string with semicolons added after each Unicode occurrence if necessary.
    &#34;&#34;&#34;
    # Define a regular expression pattern to match &#39;&amp;#x&#39; followed by hexadecimal characters
    pattern = r&#34;&amp;#x[0-9A-Fa-f]+&#34;

    def add_semicolon(match):
        unicode_value = match.group(0)
        if not unicode_value.endswith(&#34;;&#34;):
            unicode_value += &#34;;&#34;
        return unicode_value

    # Find all matches in the string using the pattern and process each match individually
    modified_string = re.sub(pattern, add_semicolon, string)

    return modified_string


def remove_spaces_between_tags(mathml_string: str) -&gt; str:
    &#34;&#34;&#34;
    Remove spaces between &#34;&gt;&#34; and &#34;&lt;&#34; in a MathML string.

    Args:
        mathml_string (str): The MathML string to process.

    Returns:
        str: The modified MathML string with spaces removed between tags.
    &#34;&#34;&#34;
    pattern = r&#34;&gt;(.*?)&lt;&#34;
    replaced_string = re.sub(
        pattern, lambda match: match.group(0).replace(&#34; &#34;, &#34;&#34;), mathml_string
    )
    return replaced_string


def render_mml(
    model: Image2MathML_Xfmer,
    vocab_itos: dict,
    vocab_stoi: dict,
    img: torch.Tensor,
    device: torch.device,
) -&gt; str:
    &#34;&#34;&#34;
    Perform sequence prediction for an input image to translate it into MathML contents.

    Args:
        model (Image2MathML_Xfmer): The image-to-MathML model.
        vocab_itos (dict): The vocabulary lookup dictionary (index to symbol).
        vocab_stoi (dict): The vocabulary lookup dictionary (symbol to index).
        img (torch.Tensor): The input image as a tensor.
        device (torch.device): The device (GPU or CPU) to be used for computation.

    Returns:
        str: The generated MathML string.
    &#34;&#34;&#34;

    model.eval()
    with torch.no_grad():
        img = img.to(device)

        output = model(
            img,
            device,
            is_inference=True,
            SOS_token=int(vocab_stoi[&#34;&lt;sos&gt;&#34;]),
            EOS_token=int(vocab_stoi[&#34;&lt;eos&gt;&#34;]),
            PAD_token=int(vocab_stoi[&#34;&lt;pad&gt;&#34;]),
        )  # O: (1, max_len, output_dim), preds: (1, max_len)

        pred = list()
        for p in output:
            pred.append(vocab_itos[str(p)])

        pred_seq = &#34; &#34;.join(pred[1:-1])
        try:
            return process_mtext(
                add_semicolon_to_unicode(remove_spaces_between_tags(pred_seq))
            )
        except:
            return add_semicolon_to_unicode(remove_spaces_between_tags(pred_seq))</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="skema.img2mml.translate.add_semicolon_to_unicode"><code class="name flex">
<span>def <span class="ident">add_semicolon_to_unicode</span></span>(<span>string: str) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Checks if the string contains Unicode starting with '&amp;#x' and adds a semicolon ';' after each occurrence if missing.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>string</code></strong> :&ensp;<code>str</code></dt>
<dd>The input string to check.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>The modified string with semicolons added after each Unicode occurrence if necessary.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_semicolon_to_unicode(string: str) -&gt; str:
    &#34;&#34;&#34;
    Checks if the string contains Unicode starting with &#39;&amp;#x&#39; and adds a semicolon &#39;;&#39; after each occurrence if missing.

    Args:
        string (str): The input string to check.

    Returns:
        str: The modified string with semicolons added after each Unicode occurrence if necessary.
    &#34;&#34;&#34;
    # Define a regular expression pattern to match &#39;&amp;#x&#39; followed by hexadecimal characters
    pattern = r&#34;&amp;#x[0-9A-Fa-f]+&#34;

    def add_semicolon(match):
        unicode_value = match.group(0)
        if not unicode_value.endswith(&#34;;&#34;):
            unicode_value += &#34;;&#34;
        return unicode_value

    # Find all matches in the string using the pattern and process each match individually
    modified_string = re.sub(pattern, add_semicolon, string)

    return modified_string</code></pre>
</details>
</dd>
<dt id="skema.img2mml.translate.calculate_scale_factor"><code class="name flex">
<span>def <span class="ident">calculate_scale_factor</span></span>(<span>image: PIL.Image.Image, target_width: int, target_height: int) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the scale factor to normalize the input image to the target width and height while preserving the
original aspect ratio. If the original aspect ratio is larger than the target aspect ratio, the scale factor
will be calculated based on width. Otherwise, it will be calculated based on height.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>image</code></strong> :&ensp;<code>PIL.Image.Image</code></dt>
<dd>The input image to be normalized.</dd>
<dt><strong><code>target_width</code></strong> :&ensp;<code>int</code></dt>
<dd>The target width for normalization.</dd>
<dt><strong><code>target_height</code></strong> :&ensp;<code>int</code></dt>
<dd>The target height for normalization.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>The scale factor to normalize the image.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculate_scale_factor(
    image: Image.Image, target_width: int, target_height: int
) -&gt; float:
    &#34;&#34;&#34;
    Calculate the scale factor to normalize the input image to the target width and height while preserving the
    original aspect ratio. If the original aspect ratio is larger than the target aspect ratio, the scale factor
    will be calculated based on width. Otherwise, it will be calculated based on height.

    Args:
        image (PIL.Image.Image): The input image to be normalized.
        target_width (int): The target width for normalization.
        target_height (int): The target height for normalization.

    Returns:
        float: The scale factor to normalize the image.
    &#34;&#34;&#34;
    original_width, original_height = image.size
    original_aspect_ratio = original_width / original_height
    target_aspect_ratio = target_width / target_height

    if original_aspect_ratio &gt; target_aspect_ratio:
        # Calculate scale factor based on width
        scale_factor = target_width / original_width
    else:
        # Calculate scale factor based on height
        scale_factor = target_height / original_height

    return scale_factor</code></pre>
</details>
</dd>
<dt id="skema.img2mml.translate.convert_to_torch_tensor"><code class="name flex">
<span>def <span class="ident">convert_to_torch_tensor</span></span>(<span>image: bytes, config: dict) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Convert image to torch tensor.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def convert_to_torch_tensor(image: bytes, config: dict) -&gt; torch.Tensor:
    &#34;&#34;&#34;Convert image to torch tensor.&#34;&#34;&#34;
    image = Image.open(io.BytesIO(image)).convert(&#34;L&#34;)
    image = preprocess_img(image, config)

    # convert to tensor
    image = transforms.ToTensor()(image)

    return image</code></pre>
</details>
</dd>
<dt id="skema.img2mml.translate.define_model"><code class="name flex">
<span>def <span class="ident">define_model</span></span>(<span>config: dict, vocab: List[str], device: torch.device, model_type='xfmer') ‑> <a title="skema.img2mml.models.image2mml_xfmer.Image2MathML_Xfmer" href="models/image2mml_xfmer.html#skema.img2mml.models.image2mml_xfmer.Image2MathML_Xfmer">Image2MathML_Xfmer</a></span>
</code></dt>
<dd>
<div class="desc"><p>Defining the model
initializing encoder, decoder, and model</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def define_model(
    config: dict, vocab: List[str], device: torch.device, model_type=&#34;xfmer&#34;
) -&gt; Image2MathML_Xfmer:
    &#34;&#34;&#34;
    Defining the model
    initializing encoder, decoder, and model
    &#34;&#34;&#34;

    print(&#34;Defining model...&#34;)

    model_type = config[&#34;model_type&#34;]
    input_channels = config[&#34;input_channels&#34;]
    output_dim = len(vocab)
    emb_dim = config[&#34;embedding_dim&#34;]
    dec_hid_dim = config[&#34;decoder_hid_dim&#34;]
    dropout = config[&#34;dropout&#34;]
    max_len = config[&#34;max_len&#34;]

    print(f&#34;building {model_type} model...&#34;)

    dim_feedfwd = config[&#34;dim_feedforward_for_xfmer&#34;]
    n_heads = config[&#34;n_xfmer_heads&#34;]
    n_xfmer_encoder_layers = config[&#34;n_xfmer_encoder_layers&#34;]
    n_xfmer_decoder_layers = config[&#34;n_xfmer_decoder_layers&#34;]
    len_dim = 2500

    enc = {
        &#34;CNN&#34;: CNN_Encoder(input_channels, dec_hid_dim, dropout, device),
        &#34;XFMER&#34;: Transformer_Encoder(
            emb_dim,
            dec_hid_dim,
            n_heads,
            dropout,
            device,
            max_len,
            n_xfmer_encoder_layers,
            dim_feedfwd,
            len_dim,
        ),
    }
    dec = Transformer_Decoder(
        emb_dim,
        n_heads,
        dec_hid_dim,
        output_dim,
        dropout,
        max_len,
        n_xfmer_decoder_layers,
        dim_feedfwd,
        device,
    )
    model = Image2MathML_Xfmer(enc, dec, vocab, device)

    return model</code></pre>
</details>
</dd>
<dt id="skema.img2mml.translate.preprocess_img"><code class="name flex">
<span>def <span class="ident">preprocess_img</span></span>(<span>image: PIL.Image.Image, config: dict) ‑> PIL.Image.Image</span>
</code></dt>
<dd>
<div class="desc"><p>preprocessing image - cropping, resizing, and padding</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def preprocess_img(image: Image.Image, config: dict) -&gt; Image.Image:
    &#34;&#34;&#34;preprocessing image - cropping, resizing, and padding&#34;&#34;&#34;
    # remove equation number if having
    image = remove_eqn_number(image)

    # converting to np array
    image_arr = np.asarray(image, dtype=np.uint8)
    # find where the data lies
    indices = np.where(image_arr != 255)
    # get the boundaries
    x_min = np.min(indices[1])
    x_max = np.max(indices[1])
    y_min = np.min(indices[0])
    y_max = np.max(indices[0])

    # cropping tha image
    image = image.crop((x_min, y_min, x_max, y_max))

    # calculate the target width and height
    target_width = config[&#34;preprocessed_image_width&#34;] - 2 * config[&#34;padding&#34;]
    target_height = config[&#34;preprocessed_image_height&#34;] - 2 * config[&#34;padding&#34;]
    # calculate the scale factor
    resize_factor = calculate_scale_factor(image, target_width, target_height)

    # resizing the image
    image = image.resize(
        (
            int(image.size[0] * resize_factor),
            int(image.size[1] * resize_factor),
        ),
        Image.LANCZOS,
    )

    # padding
    pad = config[&#34;padding&#34;]
    width = config[&#34;preprocessed_image_width&#34;]
    height = config[&#34;preprocessed_image_height&#34;]
    new_image = Image.new(&#34;RGB&#34;, (width, height), (255, 255, 255))
    new_image.paste(image, (pad, pad))

    return new_image</code></pre>
</details>
</dd>
<dt id="skema.img2mml.translate.process_mtext"><code class="name flex">
<span>def <span class="ident">process_mtext</span></span>(<span>xml_string: str) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Process the input MathML string by merging consecutive <mi> elements and replacing specific <mrow> structures.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>xml_string</code></strong> :&ensp;<code>str</code></dt>
<dd>The input MathML string.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>The modified MathML string.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process_mtext(xml_string: str) -&gt; str:
    &#34;&#34;&#34;
    Process the input MathML string by merging consecutive &lt;mi&gt; elements and replacing specific &lt;mrow&gt; structures.

    Args:
        xml_string (str): The input MathML string.

    Returns:
        str: The modified MathML string.
    &#34;&#34;&#34;
    root = ET.fromstring(xml_string)

    def merge_mi_elements(elements):
        &#34;&#34;&#34;
        Merge consecutive &lt;mi&gt; elements into an &lt;mtext&gt; element.

        Args:
            elements (list): List of consecutive &lt;mi&gt; elements.

        Returns:
            Element: The merged &lt;mtext&gt; element.
        &#34;&#34;&#34;
        merged_text = &#34;&#34;.join([elem.text for elem in elements])
        mtext = ET.Element(&#34;mtext&#34;)
        mtext.text = merged_text
        return mtext

    # Replace specific &lt;mrow&gt; structures with &lt;mtext&gt;
    for mrow in root.findall(&#34;.//mrow&#34;):
        mi_count = sum(1 for child in mrow if child.tag == &#34;mi&#34;)
        non_mi_count = sum(1 for child in mrow if child.tag != &#34;mi&#34;)

        if mi_count &gt;= 3 and non_mi_count == 0:
            mi_elements = [child for child in mrow if child.tag == &#34;mi&#34;]
            mtext = merge_mi_elements(mi_elements)
            mrow.clear()
            mrow.tag = &#34;to_be_removed&#34;
            mrow.append(mtext)

    mi_count = 0
    consecutive_mi_elements = []
    new_children = []

    # Merge consecutive &lt;mi&gt; elements
    for child in root:
        if child.tag == &#34;mi&#34;:
            mi_count += 1
            consecutive_mi_elements.append(child)
        else:
            if mi_count &gt;= 5:
                mtext = merge_mi_elements(consecutive_mi_elements)
                new_children.append(mtext)
            else:
                new_children.extend(consecutive_mi_elements)
            mi_count = 0
            consecutive_mi_elements = []
            new_children.append(child)

    if mi_count &gt;= 5:
        mtext = merge_mi_elements(consecutive_mi_elements)
        new_children.append(mtext)
    else:
        new_children.extend(consecutive_mi_elements)

    root.clear()
    root.extend(new_children)

    modified_xml_string = ET.tostring(root, encoding=&#34;unicode&#34;)
    modified_xml_string = modified_xml_string.replace(&#34;&lt;to_be_removed&gt;&#34;, &#34;&#34;)
    modified_xml_string = modified_xml_string.replace(&#34;&lt;/to_be_removed&gt;&#34;, &#34;&#34;)

    return modified_xml_string</code></pre>
</details>
</dd>
<dt id="skema.img2mml.translate.remove_eqn_number"><code class="name flex">
<span>def <span class="ident">remove_eqn_number</span></span>(<span>image: PIL.Image.Image, threshold: float = 0.1) ‑> PIL.Image.Image</span>
</code></dt>
<dd>
<div class="desc"><p>Remove equation number from an image of an equation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>image</code></strong> :&ensp;<code>Image.Image</code></dt>
<dd>The input image.</dd>
<dt><strong><code>threshold</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>The threshold to determine the size of the equation number.
A smaller threshold will consider larger areas as equation numbers.
Defaults to 0.1.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Image.Image</code></dt>
<dd>The modified image with the equation number removed.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remove_eqn_number(image: Image.Image, threshold: float = 0.1) -&gt; Image.Image:
    &#34;&#34;&#34;
    Remove equation number from an image of an equation.

    Args:
        image (Image.Image): The input image.
        threshold (float, optional): The threshold to determine the size of the equation number.
            A smaller threshold will consider larger areas as equation numbers.
            Defaults to 0.1.

    Returns:
        Image.Image: The modified image with the equation number removed.
    &#34;&#34;&#34;
    image_arr = np.asarray(image, dtype=np.uint8)

    # Invert the image by subtracting it from the maximum pixel value
    inverted = np.max(image_arr) - image_arr

    # Get the width and height of the image
    height, width = inverted.shape[:2]

    # Start scanning from the right side
    column_sum = np.sum(inverted, axis=0)
    rightmost_column = width - 1
    leftmost_column = rightmost_column
    while leftmost_column &gt;= 0:
        if column_sum[leftmost_column] != 0:
            if rightmost_column - leftmost_column &gt; threshold * width:
                image_arr = image_arr[:, 0:leftmost_column]
                return Image.fromarray(image_arr)

            leftmost_column -= 1
            rightmost_column = leftmost_column
        else:
            leftmost_column -= 1

    return Image.fromarray(image_arr)</code></pre>
</details>
</dd>
<dt id="skema.img2mml.translate.remove_spaces_between_tags"><code class="name flex">
<span>def <span class="ident">remove_spaces_between_tags</span></span>(<span>mathml_string: str) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Remove spaces between "&gt;" and "&lt;" in a MathML string.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>mathml_string</code></strong> :&ensp;<code>str</code></dt>
<dd>The MathML string to process.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>The modified MathML string with spaces removed between tags.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remove_spaces_between_tags(mathml_string: str) -&gt; str:
    &#34;&#34;&#34;
    Remove spaces between &#34;&gt;&#34; and &#34;&lt;&#34; in a MathML string.

    Args:
        mathml_string (str): The MathML string to process.

    Returns:
        str: The modified MathML string with spaces removed between tags.
    &#34;&#34;&#34;
    pattern = r&#34;&gt;(.*?)&lt;&#34;
    replaced_string = re.sub(
        pattern, lambda match: match.group(0).replace(&#34; &#34;, &#34;&#34;), mathml_string
    )
    return replaced_string</code></pre>
</details>
</dd>
<dt id="skema.img2mml.translate.render_mml"><code class="name flex">
<span>def <span class="ident">render_mml</span></span>(<span>model: <a title="skema.img2mml.models.image2mml_xfmer.Image2MathML_Xfmer" href="models/image2mml_xfmer.html#skema.img2mml.models.image2mml_xfmer.Image2MathML_Xfmer">Image2MathML_Xfmer</a>, vocab_itos: dict, vocab_stoi: dict, img: torch.Tensor, device: torch.device) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Perform sequence prediction for an input image to translate it into MathML contents.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>Image2MathML_Xfmer</code></dt>
<dd>The image-to-MathML model.</dd>
<dt><strong><code>vocab_itos</code></strong> :&ensp;<code>dict</code></dt>
<dd>The vocabulary lookup dictionary (index to symbol).</dd>
<dt><strong><code>vocab_stoi</code></strong> :&ensp;<code>dict</code></dt>
<dd>The vocabulary lookup dictionary (symbol to index).</dd>
<dt><strong><code>img</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The input image as a tensor.</dd>
<dt><strong><code>device</code></strong> :&ensp;<code>torch.device</code></dt>
<dd>The device (GPU or CPU) to be used for computation.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>The generated MathML string.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def render_mml(
    model: Image2MathML_Xfmer,
    vocab_itos: dict,
    vocab_stoi: dict,
    img: torch.Tensor,
    device: torch.device,
) -&gt; str:
    &#34;&#34;&#34;
    Perform sequence prediction for an input image to translate it into MathML contents.

    Args:
        model (Image2MathML_Xfmer): The image-to-MathML model.
        vocab_itos (dict): The vocabulary lookup dictionary (index to symbol).
        vocab_stoi (dict): The vocabulary lookup dictionary (symbol to index).
        img (torch.Tensor): The input image as a tensor.
        device (torch.device): The device (GPU or CPU) to be used for computation.

    Returns:
        str: The generated MathML string.
    &#34;&#34;&#34;

    model.eval()
    with torch.no_grad():
        img = img.to(device)

        output = model(
            img,
            device,
            is_inference=True,
            SOS_token=int(vocab_stoi[&#34;&lt;sos&gt;&#34;]),
            EOS_token=int(vocab_stoi[&#34;&lt;eos&gt;&#34;]),
            PAD_token=int(vocab_stoi[&#34;&lt;pad&gt;&#34;]),
        )  # O: (1, max_len, output_dim), preds: (1, max_len)

        pred = list()
        for p in output:
            pred.append(vocab_itos[str(p)])

        pred_seq = &#34; &#34;.join(pred[1:-1])
        try:
            return process_mtext(
                add_semicolon_to_unicode(remove_spaces_between_tags(pred_seq))
            )
        except:
            return add_semicolon_to_unicode(remove_spaces_between_tags(pred_seq))</code></pre>
</details>
</dd>
<dt id="skema.img2mml.translate.set_random_seed"><code class="name flex">
<span>def <span class="ident">set_random_seed</span></span>(<span>seed: int) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Set up seed.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_random_seed(seed: int) -&gt; None:
    &#34;&#34;&#34;Set up seed.&#34;&#34;&#34;

    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="skema.img2mml" href="index.html">skema.img2mml</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="skema.img2mml.translate.add_semicolon_to_unicode" href="#skema.img2mml.translate.add_semicolon_to_unicode">add_semicolon_to_unicode</a></code></li>
<li><code><a title="skema.img2mml.translate.calculate_scale_factor" href="#skema.img2mml.translate.calculate_scale_factor">calculate_scale_factor</a></code></li>
<li><code><a title="skema.img2mml.translate.convert_to_torch_tensor" href="#skema.img2mml.translate.convert_to_torch_tensor">convert_to_torch_tensor</a></code></li>
<li><code><a title="skema.img2mml.translate.define_model" href="#skema.img2mml.translate.define_model">define_model</a></code></li>
<li><code><a title="skema.img2mml.translate.preprocess_img" href="#skema.img2mml.translate.preprocess_img">preprocess_img</a></code></li>
<li><code><a title="skema.img2mml.translate.process_mtext" href="#skema.img2mml.translate.process_mtext">process_mtext</a></code></li>
<li><code><a title="skema.img2mml.translate.remove_eqn_number" href="#skema.img2mml.translate.remove_eqn_number">remove_eqn_number</a></code></li>
<li><code><a title="skema.img2mml.translate.remove_spaces_between_tags" href="#skema.img2mml.translate.remove_spaces_between_tags">remove_spaces_between_tags</a></code></li>
<li><code><a title="skema.img2mml.translate.render_mml" href="#skema.img2mml.translate.render_mml">render_mml</a></code></li>
<li><code><a title="skema.img2mml.translate.set_random_seed" href="#skema.img2mml.translate.set_random_seed">set_random_seed</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>