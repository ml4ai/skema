<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>skema.text_reading.mention_linking.gromet_linker.text_reading_linker API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>skema.text_reading.mention_linking.gromet_linker.text_reading_linker</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import itertools
import json
from pathlib import Path
from typing import Any, Dict, Optional
from collections import defaultdict
from gensim.models import KeyedVectors
from typing import List, Union
import numpy as np
import itertools as it
from transformers import *
import torch


class TextReadingLinker:
    &#34;&#34;&#34;Encapsulates the logic of the linker to text reading mentions&#34;&#34;&#34;

    def __init__(self, mentions_path: str, embeddings_path: Optional[str]):

        # Load the embeddings if embeddings path, otherwise use scibert
        # if embeddings_path:
        #     self._model = KeyedVectors.load(embeddings_path)
        # else:
        if torch.cuda.is_available():
            device = &#39;cuda:1&#39; # TODO parameterize this
        else:
            device = &#39;cpu&#39;
        self._device = device
        self._tokenizer = AutoTokenizer.from_pretrained(&#39;allenai/scibert_scivocab_uncased&#39;)
        self._model = AutoModel.from_pretrained(&#39;allenai/scibert_scivocab_uncased&#39;).to(device).eval()
        

        # Read the mentions
        raw_mentions, tb_mentions, documents = self._read_text_mentions(
            mentions_path
        )
        self._docs = documents
        self._tb_mentions = tb_mentions

        linkable_mentions = [
            m
            for m in raw_mentions
            if any(&#34;variable&#34; in a for a in m[&#34;arguments&#34;])
        ]
        self._linkable_mentions = linkable_mentions
        self._linkable_variables = defaultdict(list)
        self._linkable_descriptions = dict()

        for m in linkable_mentions:
            arg = m[&#34;arguments&#34;][&#34;variable&#34;][0]
            var = arg[&#34;text&#34;]
            context = (tuple(m[&#39;context&#39;].split()), m[&#39;tokenInterval&#39;][&#39;start&#39;], m[&#39;tokenInterval&#39;][&#39;end&#39;]-1)
            
            if type(self._model) == KeyedVectors and len(self._preprocess(var)) &gt; 0:
                self._linkable_variables[(var, context)].append(m)
            else:
                self._linkable_variables[(var, context)].append(m)

            # This is a level of indirection in which we also look at the variable descriptions for linking
            if &#34;description&#34; in m[&#34;arguments&#34;]:
                desc = m[&#34;arguments&#34;][&#34;description&#34;][0][&#34;text&#34;]

                if type(self._model) == KeyedVectors and len(self._preprocess(desc)) &gt; 0:
                    self._linkable_descriptions[
                        (desc, context)
                    ] = var  # We resolve to the variable name, which will use later to do the &#34;graph&#34; linking
                else:
                    self._linkable_descriptions[
                        (desc, context)
                    ] = var  # We resolve to the variable name, which will use later to do the &#34;graph&#34; linking


        # Preprocess the vectors for each mention text
        keys, vectors = list(), list()
        if type(self._model) == KeyedVectors:
            agg_function = lambda t, s, e: self._average_vector(self._preprocess(&#39; &#39;.join(t)))
        else:
            agg_function = self._contextualized_vector

        for (k, ctx) in it.chain(
            self._linkable_variables, self._linkable_descriptions
        ):
            keys.append((k, ctx))
            # TODO make the agg_function ctx aware
            tokens, start, end = ctx
            vectors.append(agg_function(tokens, start, end))

        vectors = np.stack(vectors, axis=0)

        self._keys = keys
        self._vectors = vectors

        # TODO Remove after this is fixed in TR
        self._fix_groundings()

    def _preprocess(self, text: Union[str, List[str]]) -&gt; List[str]:
        &#34;&#34;&#34;Prepares the text for before fetching embeddings&#34;&#34;&#34;
        if type(text) == str:
            text = [text]

        return [
            word
            for word in itertools.chain.from_iterable(
                sent.split() for sent in text
            )
            if word in self._model
        ]

    def _read_text_mentions(self, input_path: str) -&gt; Dict[str, Any]:

        # TODO Filter out irrelevant extractions
        relevant_labels = {
            # For ports
            &#34;Parameter&#34;,
            &#34;ParamAndUnit&#34;,
            &#34;GreekLetter&#34;,
            &#34;Model&#34;,
            &#34;model&#34;,
            &#34;ParameterSetting&#34;,
            &#34;ModelComponent&#34;,
            # For box functions
            &#34;Function&#34;,
            &#34;ParameterSetting&#34;,
            &#34;UnitRelation&#34;,
        }

        input_path = Path(input_path)

        if not input_path.is_dir():
            files =  [input_path]
        else:
            files = input_path.glob(&#34;*.json&#34;)

        relevant_mentions, text_bound_mentions, docs = list(), list(), dict()

        for file in files:

            doc_name = file.name

            with file.open() as f:
                data = json.load(f)

            local_relevant_mentions = [
                m
                for m in data[&#34;mentions&#34;]
                if m[&#34;type&#34;] != &#34;TextBoundMention&#34;
                and len(set(m[&#34;labels&#34;]) &amp; relevant_labels) &gt; 0
            ]

            # Fix the mention document
            for m in local_relevant_mentions:
                m[&#39;document&#39;] = (doc_name, m.get(&#39;document&#39;, &#34;N/A&#34;))

            local_text_bound_mentions = list(
                it.chain.from_iterable(
                    it.chain.from_iterable(
                        m[&#34;arguments&#34;].values() for m in local_relevant_mentions
                    )
                )
            )

            # Add context to the mentions
            local_docs = data[&#34;documents&#34;]
            if len(local_docs) &gt; 0:
                for m in local_relevant_mentions:
                    doc = local_docs[m[&#34;document&#34;][1]]
                    sent = doc[&#34;sentences&#34;][m[&#34;sentence&#34;]]
                    # TODO perhaps extend this to a window of text
                    context = &#34; &#34;.join(sent[&#34;raw&#34;])
                    m[&#34;context&#34;] = context

            relevant_mentions.extend(local_relevant_mentions)
            text_bound_mentions.extend(local_text_bound_mentions)
            docs.update({(doc_name, key):value for key, value in local_docs.items()})

        print(len(relevant_mentions))

        return relevant_mentions, text_bound_mentions, docs

    def _contextualized_vector(self, input_text: Union[List[str], str], start: int = 0, end: int = - 1):
        &#34;&#34;&#34;Computes the contextualized vector using a bert model for cosine similarity&#34;&#34;&#34;

        # Tokenize the input
        if type(input_text) != str:
            if len(input_text) == 1:
                end = start 
            input = self._tokenizer(input_text, is_split_into_words=True, return_tensors=&#39;pt&#39;).to(self._device)
            # Map word indices to sub word tokens
            start = input.word_to_tokens(start).start
            end = (input.word_to_tokens(end).end)
        else:
            input = self._tokenizer(input_text, return_tensors=&#39;pt&#39;).to(self._device)
            start, end = 1, -1

        # Forward pass
        output = self._model(**input).last_hidden_state[0, :, :]
        # Select the first and last token of the mention
        first, last = output[start, :].detach().cpu().numpy(), output[end-1, :].detach().cpu().numpy()
        emb = np.concatenate([first, last])

        return emb

    def _average_vector(self, words: List[str]):
        &#34;&#34;&#34;Precomputes and l2 normalizes the average vector of the requested word embeddings&#34;&#34;&#34;

        vectors = self._model[words]
        avg = vectors.mean(axis=0)
        norm = np.linalg.norm(avg)
        normalized_avg = avg / norm
        return normalized_avg

    def align_to_comments(self, comments, threshold=0.5, k=10):

        if type(self._model) == KeyedVectors:
            tokens = self._preprocess(comments)
            if len(tokens) &gt; 0:
                emb = self._average_vector(tokens)
            else:
                return []

        else:
            if len(comments) &gt; 0:
                emb = self._contextualized_vector(comments)
            else:
                return []
            

        similarities = self._vectors @ emb
        similarities = similarities[similarities &gt;= threshold]
        if k &gt; self._vectors.shape[0]:
            k = self._vectors.shape[0]
        topk = np.argsort(-1 * similarities)[:k]
        # We have to account for variables and descriptions
        chosen_mentions = list()
        for i in topk:
            key = self._keys[i]
            score = similarities[i]
            if key in self._linkable_variables:
                var = key
            else:
                var = self._linkable_descriptions[key]
            for m in self._linkable_variables[var]:
                chosen_mentions.append((m, score))
        return chosen_mentions

    @property
    def documents(self):
        return self._docs

    def _fix_groundings(self):
        &#34;&#34;&#34;This is a temporary fix to restore the missing groundings on the arguments of the events and relations. This will be obsolete once this issue is fixed in the TR pipeline&#34;&#34;&#34;

        # Find the text bound mentions
        tb_mentions = {m[&#34;id&#34;]: m for m in self._tb_mentions}

        # Iterate over the arguments and restore the attachments
        for m in self._linkable_mentions:
            if m[&#34;type&#34;] != &#34;TextBoundMention&#34;:
                for arg in m[&#34;arguments&#34;].values():
                    arg = arg[0]
                    id = arg[&#34;id&#34;]
                    if id in tb_mentions:
                        tb = tb_mentions[id]
                        arg[&#34;attachments&#34;] = tb[&#34;attachments&#34;]</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="skema.text_reading.mention_linking.gromet_linker.text_reading_linker.TextReadingLinker"><code class="flex name class">
<span>class <span class="ident">TextReadingLinker</span></span>
<span>(</span><span>mentions_path: str, embeddings_path: Optional[str])</span>
</code></dt>
<dd>
<div class="desc"><p>Encapsulates the logic of the linker to text reading mentions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TextReadingLinker:
    &#34;&#34;&#34;Encapsulates the logic of the linker to text reading mentions&#34;&#34;&#34;

    def __init__(self, mentions_path: str, embeddings_path: Optional[str]):

        # Load the embeddings if embeddings path, otherwise use scibert
        # if embeddings_path:
        #     self._model = KeyedVectors.load(embeddings_path)
        # else:
        if torch.cuda.is_available():
            device = &#39;cuda:1&#39; # TODO parameterize this
        else:
            device = &#39;cpu&#39;
        self._device = device
        self._tokenizer = AutoTokenizer.from_pretrained(&#39;allenai/scibert_scivocab_uncased&#39;)
        self._model = AutoModel.from_pretrained(&#39;allenai/scibert_scivocab_uncased&#39;).to(device).eval()
        

        # Read the mentions
        raw_mentions, tb_mentions, documents = self._read_text_mentions(
            mentions_path
        )
        self._docs = documents
        self._tb_mentions = tb_mentions

        linkable_mentions = [
            m
            for m in raw_mentions
            if any(&#34;variable&#34; in a for a in m[&#34;arguments&#34;])
        ]
        self._linkable_mentions = linkable_mentions
        self._linkable_variables = defaultdict(list)
        self._linkable_descriptions = dict()

        for m in linkable_mentions:
            arg = m[&#34;arguments&#34;][&#34;variable&#34;][0]
            var = arg[&#34;text&#34;]
            context = (tuple(m[&#39;context&#39;].split()), m[&#39;tokenInterval&#39;][&#39;start&#39;], m[&#39;tokenInterval&#39;][&#39;end&#39;]-1)
            
            if type(self._model) == KeyedVectors and len(self._preprocess(var)) &gt; 0:
                self._linkable_variables[(var, context)].append(m)
            else:
                self._linkable_variables[(var, context)].append(m)

            # This is a level of indirection in which we also look at the variable descriptions for linking
            if &#34;description&#34; in m[&#34;arguments&#34;]:
                desc = m[&#34;arguments&#34;][&#34;description&#34;][0][&#34;text&#34;]

                if type(self._model) == KeyedVectors and len(self._preprocess(desc)) &gt; 0:
                    self._linkable_descriptions[
                        (desc, context)
                    ] = var  # We resolve to the variable name, which will use later to do the &#34;graph&#34; linking
                else:
                    self._linkable_descriptions[
                        (desc, context)
                    ] = var  # We resolve to the variable name, which will use later to do the &#34;graph&#34; linking


        # Preprocess the vectors for each mention text
        keys, vectors = list(), list()
        if type(self._model) == KeyedVectors:
            agg_function = lambda t, s, e: self._average_vector(self._preprocess(&#39; &#39;.join(t)))
        else:
            agg_function = self._contextualized_vector

        for (k, ctx) in it.chain(
            self._linkable_variables, self._linkable_descriptions
        ):
            keys.append((k, ctx))
            # TODO make the agg_function ctx aware
            tokens, start, end = ctx
            vectors.append(agg_function(tokens, start, end))

        vectors = np.stack(vectors, axis=0)

        self._keys = keys
        self._vectors = vectors

        # TODO Remove after this is fixed in TR
        self._fix_groundings()

    def _preprocess(self, text: Union[str, List[str]]) -&gt; List[str]:
        &#34;&#34;&#34;Prepares the text for before fetching embeddings&#34;&#34;&#34;
        if type(text) == str:
            text = [text]

        return [
            word
            for word in itertools.chain.from_iterable(
                sent.split() for sent in text
            )
            if word in self._model
        ]

    def _read_text_mentions(self, input_path: str) -&gt; Dict[str, Any]:

        # TODO Filter out irrelevant extractions
        relevant_labels = {
            # For ports
            &#34;Parameter&#34;,
            &#34;ParamAndUnit&#34;,
            &#34;GreekLetter&#34;,
            &#34;Model&#34;,
            &#34;model&#34;,
            &#34;ParameterSetting&#34;,
            &#34;ModelComponent&#34;,
            # For box functions
            &#34;Function&#34;,
            &#34;ParameterSetting&#34;,
            &#34;UnitRelation&#34;,
        }

        input_path = Path(input_path)

        if not input_path.is_dir():
            files =  [input_path]
        else:
            files = input_path.glob(&#34;*.json&#34;)

        relevant_mentions, text_bound_mentions, docs = list(), list(), dict()

        for file in files:

            doc_name = file.name

            with file.open() as f:
                data = json.load(f)

            local_relevant_mentions = [
                m
                for m in data[&#34;mentions&#34;]
                if m[&#34;type&#34;] != &#34;TextBoundMention&#34;
                and len(set(m[&#34;labels&#34;]) &amp; relevant_labels) &gt; 0
            ]

            # Fix the mention document
            for m in local_relevant_mentions:
                m[&#39;document&#39;] = (doc_name, m.get(&#39;document&#39;, &#34;N/A&#34;))

            local_text_bound_mentions = list(
                it.chain.from_iterable(
                    it.chain.from_iterable(
                        m[&#34;arguments&#34;].values() for m in local_relevant_mentions
                    )
                )
            )

            # Add context to the mentions
            local_docs = data[&#34;documents&#34;]
            if len(local_docs) &gt; 0:
                for m in local_relevant_mentions:
                    doc = local_docs[m[&#34;document&#34;][1]]
                    sent = doc[&#34;sentences&#34;][m[&#34;sentence&#34;]]
                    # TODO perhaps extend this to a window of text
                    context = &#34; &#34;.join(sent[&#34;raw&#34;])
                    m[&#34;context&#34;] = context

            relevant_mentions.extend(local_relevant_mentions)
            text_bound_mentions.extend(local_text_bound_mentions)
            docs.update({(doc_name, key):value for key, value in local_docs.items()})

        print(len(relevant_mentions))

        return relevant_mentions, text_bound_mentions, docs

    def _contextualized_vector(self, input_text: Union[List[str], str], start: int = 0, end: int = - 1):
        &#34;&#34;&#34;Computes the contextualized vector using a bert model for cosine similarity&#34;&#34;&#34;

        # Tokenize the input
        if type(input_text) != str:
            if len(input_text) == 1:
                end = start 
            input = self._tokenizer(input_text, is_split_into_words=True, return_tensors=&#39;pt&#39;).to(self._device)
            # Map word indices to sub word tokens
            start = input.word_to_tokens(start).start
            end = (input.word_to_tokens(end).end)
        else:
            input = self._tokenizer(input_text, return_tensors=&#39;pt&#39;).to(self._device)
            start, end = 1, -1

        # Forward pass
        output = self._model(**input).last_hidden_state[0, :, :]
        # Select the first and last token of the mention
        first, last = output[start, :].detach().cpu().numpy(), output[end-1, :].detach().cpu().numpy()
        emb = np.concatenate([first, last])

        return emb

    def _average_vector(self, words: List[str]):
        &#34;&#34;&#34;Precomputes and l2 normalizes the average vector of the requested word embeddings&#34;&#34;&#34;

        vectors = self._model[words]
        avg = vectors.mean(axis=0)
        norm = np.linalg.norm(avg)
        normalized_avg = avg / norm
        return normalized_avg

    def align_to_comments(self, comments, threshold=0.5, k=10):

        if type(self._model) == KeyedVectors:
            tokens = self._preprocess(comments)
            if len(tokens) &gt; 0:
                emb = self._average_vector(tokens)
            else:
                return []

        else:
            if len(comments) &gt; 0:
                emb = self._contextualized_vector(comments)
            else:
                return []
            

        similarities = self._vectors @ emb
        similarities = similarities[similarities &gt;= threshold]
        if k &gt; self._vectors.shape[0]:
            k = self._vectors.shape[0]
        topk = np.argsort(-1 * similarities)[:k]
        # We have to account for variables and descriptions
        chosen_mentions = list()
        for i in topk:
            key = self._keys[i]
            score = similarities[i]
            if key in self._linkable_variables:
                var = key
            else:
                var = self._linkable_descriptions[key]
            for m in self._linkable_variables[var]:
                chosen_mentions.append((m, score))
        return chosen_mentions

    @property
    def documents(self):
        return self._docs

    def _fix_groundings(self):
        &#34;&#34;&#34;This is a temporary fix to restore the missing groundings on the arguments of the events and relations. This will be obsolete once this issue is fixed in the TR pipeline&#34;&#34;&#34;

        # Find the text bound mentions
        tb_mentions = {m[&#34;id&#34;]: m for m in self._tb_mentions}

        # Iterate over the arguments and restore the attachments
        for m in self._linkable_mentions:
            if m[&#34;type&#34;] != &#34;TextBoundMention&#34;:
                for arg in m[&#34;arguments&#34;].values():
                    arg = arg[0]
                    id = arg[&#34;id&#34;]
                    if id in tb_mentions:
                        tb = tb_mentions[id]
                        arg[&#34;attachments&#34;] = tb[&#34;attachments&#34;]</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="skema.text_reading.mention_linking.gromet_linker.text_reading_linker.TextReadingLinker.documents"><code class="name">var <span class="ident">documents</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def documents(self):
    return self._docs</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="skema.text_reading.mention_linking.gromet_linker.text_reading_linker.TextReadingLinker.align_to_comments"><code class="name flex">
<span>def <span class="ident">align_to_comments</span></span>(<span>self, comments, threshold=0.5, k=10)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def align_to_comments(self, comments, threshold=0.5, k=10):

    if type(self._model) == KeyedVectors:
        tokens = self._preprocess(comments)
        if len(tokens) &gt; 0:
            emb = self._average_vector(tokens)
        else:
            return []

    else:
        if len(comments) &gt; 0:
            emb = self._contextualized_vector(comments)
        else:
            return []
        

    similarities = self._vectors @ emb
    similarities = similarities[similarities &gt;= threshold]
    if k &gt; self._vectors.shape[0]:
        k = self._vectors.shape[0]
    topk = np.argsort(-1 * similarities)[:k]
    # We have to account for variables and descriptions
    chosen_mentions = list()
    for i in topk:
        key = self._keys[i]
        score = similarities[i]
        if key in self._linkable_variables:
            var = key
        else:
            var = self._linkable_descriptions[key]
        for m in self._linkable_variables[var]:
            chosen_mentions.append((m, score))
    return chosen_mentions</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="skema.text_reading.mention_linking.gromet_linker" href="index.html">skema.text_reading.mention_linking.gromet_linker</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="skema.text_reading.mention_linking.gromet_linker.text_reading_linker.TextReadingLinker" href="#skema.text_reading.mention_linking.gromet_linker.text_reading_linker.TextReadingLinker">TextReadingLinker</a></code></h4>
<ul class="">
<li><code><a title="skema.text_reading.mention_linking.gromet_linker.text_reading_linker.TextReadingLinker.align_to_comments" href="#skema.text_reading.mention_linking.gromet_linker.text_reading_linker.TextReadingLinker.align_to_comments">align_to_comments</a></code></li>
<li><code><a title="skema.text_reading.mention_linking.gromet_linker.text_reading_linker.TextReadingLinker.documents" href="#skema.text_reading.mention_linking.gromet_linker.text_reading_linker.TextReadingLinker.documents">documents</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>